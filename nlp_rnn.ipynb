{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRBJ/e6Y2gS5QXWr2KYool",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/payal500/practice-series-/blob/main/nlp_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN**- Recurent neural network is a concpet we use for sequential data like time series, sentiment analysis. It is different from ANN because its have feedback concpet used in hidden layer that make it different from ANN .\n",
        "\n",
        "We take feedback of ist word and pass it as input with second word so it remeber the previous word and maintan the sequence of sentence\n",
        "\n",
        "we have time stamp and word (t11) first we take vetocr of first word and pass as input in hidden layer(x11) where we do\n",
        "dot product of weight (w1)= f(x(11).(w1) + ot-1.wh ) .\n",
        "ot is output of previous word and weight of hidden layer and we apply tanh function (f)\n"
      ],
      "metadata": {
        "id": "a628Wf_h6dHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**we can convert word into vector using two techniques:**\n",
        "\n",
        "\n",
        "*   integer encoding - in this convert unique word into number series ex 1,2,3 and if 2 sentence lenght is more then it add 0 on other row   \n",
        "\n",
        " 1st sentence :Hi there [1 2 0 0]\n",
        "\n",
        "                \n",
        "  2 senctence :  my name is payal [3 4 5 6]\n",
        "\n",
        "*   embeding : non -zero value like 0.2,0.8 ; dense represetation; capture sementic meaning ; low dimensional\n",
        "\n"
      ],
      "metadata": {
        "id": "o30TCX9u6229"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# integer encoding hands on"
      ],
      "metadata": {
        "id": "uLEFYnDeLtMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "zwB09swM78Zb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=['go india go',\n",
        "     'hi payal ',\n",
        "     'how are you',\n",
        "     ' india will win the match',\n",
        "     'do you love cricket ',\n",
        "     'i donot like cricket match but like virat kolhi',\n",
        "     'india india india',\n",
        "     'india is the best',\n",
        "     'india is the best in the world',\n",
        "     'we proud to be indian ',\n",
        "     'we hate outdoor games'\n",
        "     ]\n",
        ""
      ],
      "metadata": {
        "id": "YMaAFWjoGcCl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#oov token is used when we have now sentence and word not match to our doc so it replace word to null or anything we can give in ''\n",
        "tokenizer=Tokenizer(oov_token='null')\n",
        "tokenizer.fit_on_texts(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40ZXPLGDHKvH",
        "outputId": "01c482c0-f712-4550-93d6-3f6f1d09370a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#it gives index value of every unique word in doc\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffI6y7QpH5cD",
        "outputId": "c34ab729-32ea-4ef3-e7f5-6ed2f6e3398c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'null': 1,\n",
              " 'india': 2,\n",
              " 'the': 3,\n",
              " 'go': 4,\n",
              " 'you': 5,\n",
              " 'match': 6,\n",
              " 'cricket': 7,\n",
              " 'like': 8,\n",
              " 'is': 9,\n",
              " 'best': 10,\n",
              " 'we': 11,\n",
              " 'hi': 12,\n",
              " 'payal': 13,\n",
              " 'how': 14,\n",
              " 'are': 15,\n",
              " 'will': 16,\n",
              " 'win': 17,\n",
              " 'do': 18,\n",
              " 'love': 19,\n",
              " 'i': 20,\n",
              " 'donot': 21,\n",
              " 'but': 22,\n",
              " 'virat': 23,\n",
              " 'kolhi': 24,\n",
              " 'in': 25,\n",
              " 'world': 26,\n",
              " 'proud': 27,\n",
              " 'to': 28,\n",
              " 'be': 29,\n",
              " 'indian': 30,\n",
              " 'hate': 31,\n",
              " 'outdoor': 32,\n",
              " 'games': 33}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#it will show the count of total unique words\n",
        "tokenizer.word_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcxd-2JhJoLi",
        "outputId": "3b0e4ff3-7255-403f-82ed-4f00f9b3f431"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('go', 2),\n",
              "             ('india', 7),\n",
              "             ('hi', 1),\n",
              "             ('payal', 1),\n",
              "             ('how', 1),\n",
              "             ('are', 1),\n",
              "             ('you', 2),\n",
              "             ('will', 1),\n",
              "             ('win', 1),\n",
              "             ('the', 4),\n",
              "             ('match', 2),\n",
              "             ('do', 1),\n",
              "             ('love', 1),\n",
              "             ('cricket', 2),\n",
              "             ('i', 1),\n",
              "             ('donot', 1),\n",
              "             ('like', 2),\n",
              "             ('but', 1),\n",
              "             ('virat', 1),\n",
              "             ('kolhi', 1),\n",
              "             ('is', 2),\n",
              "             ('best', 2),\n",
              "             ('in', 1),\n",
              "             ('world', 1),\n",
              "             ('we', 2),\n",
              "             ('proud', 1),\n",
              "             ('to', 1),\n",
              "             ('be', 1),\n",
              "             ('indian', 1),\n",
              "             ('hate', 1),\n",
              "             ('outdoor', 1),\n",
              "             ('games', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#shows the toptal no. of sentence\n",
        "tokenizer.document_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw0KjHceJyeX",
        "outputId": "dcd3f231-0ef7-4759-8e9c-c074d5b99a1e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# it give token/sequence no. to all words according to initalize in fit_on_text\n",
        "sequence=tokenizer.texts_to_sequences(doc)\n",
        "sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sdIkBMyJ8yi",
        "outputId": "6a4cc4f7-e04d-4776-9e08-423d1ee5f865"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 2, 4],\n",
              " [12, 13],\n",
              " [14, 15, 5],\n",
              " [2, 16, 17, 3, 6],\n",
              " [18, 5, 19, 7],\n",
              " [20, 21, 8, 7, 6, 22, 8, 23, 24],\n",
              " [2, 2, 2],\n",
              " [2, 9, 3, 10],\n",
              " [2, 9, 3, 10, 25, 3, 26],\n",
              " [11, 27, 28, 29, 30],\n",
              " [11, 31, 32, 33]]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pad_sequence is used to add 0 on data so all row will be same lenght\n",
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "5w5ZbKvEKMnb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#padding= post meaning add 0 in last of data\n",
        "sequence=pad_sequences(sequence,padding='post')\n",
        "sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlD08K4yLWvN",
        "outputId": "9bfcedf3-d429-4b8f-d715-708907e0ca29"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4,  2,  4,  0,  0,  0,  0,  0,  0],\n",
              "       [12, 13,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [14, 15,  5,  0,  0,  0,  0,  0,  0],\n",
              "       [ 2, 16, 17,  3,  6,  0,  0,  0,  0],\n",
              "       [18,  5, 19,  7,  0,  0,  0,  0,  0],\n",
              "       [20, 21,  8,  7,  6, 22,  8, 23, 24],\n",
              "       [ 2,  2,  2,  0,  0,  0,  0,  0,  0],\n",
              "       [ 2,  9,  3, 10,  0,  0,  0,  0,  0],\n",
              "       [ 2,  9,  3, 10, 25,  3, 26,  0,  0],\n",
              "       [11, 27, 28, 29, 30,  0,  0,  0,  0],\n",
              "       [11, 31, 32, 33,  0,  0,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ersau"
      ],
      "metadata": {
        "id": "8xb6f17QLlyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sentiment analysis using keras\n",
        "In keras.dataset intereger encoding alerady done  "
      ],
      "metadata": {
        "id": "mRK8J-TgMGje"
      }
    }
  ]
}